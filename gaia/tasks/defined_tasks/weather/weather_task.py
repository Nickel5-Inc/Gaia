import asyncio
import traceback
from typing import Any, Dict, List, Optional, Union, Tuple
from uuid import uuid4
from datetime import datetime, timezone, timedelta
import os
import importlib.util
import json
from pydantic import Field, ConfigDict
from fiber.logging_utils import get_logger
from gaia.tasks.base.task import Task
from gaia.validator.database.validator_database_manager import ValidatorDatabaseManager
from gaia.miner.database.miner_database_manager import MinerDatabaseManager
import uuid
import time
from pathlib import Path
import xarray as xr
import pickle
import base64
import jwt
import numpy as np
import torch
import fsspec
from kerchunk.hdf import SingleHdf5ToZarr
import pandas as pd
from cryptography.fernet import Fernet
from fiber.encrypted.validator import handshake
from fiber.encrypted.validator import client as vali_client
from sqlalchemy import text, bindparam, TEXT
import httpx
import gc
import shutil
import zarr
import numcodecs
from typing import Dict
import gzip
import tarfile
import io
import tempfile
import subprocess

import boto3 # For R2
from botocore.exceptions import ClientError as BotoClientError # For R2 error handling, aliased to avoid conflict if other ClientError exists
from . import weather_http_utils

from .utils.era5_api import fetch_era5_data
from .utils.gfs_api import fetch_gfs_analysis_data, fetch_gfs_data
from .utils.hashing import compute_verification_hash, compute_input_data_hash
from .utils.kerchunk_utils import generate_kerchunk_json_from_local_file
from .utils.data_prep import create_aurora_batch_from_gfs
from .schemas.weather_metadata import WeatherMetadata
from .schemas.weather_inputs import WeatherInputs, WeatherForecastRequest, WeatherInputData, WeatherInitiateFetchData, WeatherGetInputStatusData, WeatherStartInferenceData
from .schemas.weather_outputs import WeatherOutputs, WeatherKerchunkResponseData
from .weather_scoring.metrics import calculate_rmse
from .processing.weather_miner_preprocessing import prepare_miner_batch_from_payload
from .processing.weather_logic import (
    _update_run_status, build_score_row, get_ground_truth_data,
    _trigger_initial_scoring, _request_fresh_token, verify_miner_response,
    get_job_by_gfs_init_time, update_job_status, update_job_paths
)
from .processing.weather_workers import (
    initial_scoring_worker, 
    finalize_scores_worker, 
    cleanup_worker,
    run_inference_background,
    fetch_and_hash_gfs_task,
    r2_cleanup_worker,
    poll_runpod_job_worker
)
from gaia.tasks.defined_tasks.weather.utils.inference_class import WeatherInferenceRunner
logger = get_logger(__name__)
from aurora import Batch

DEFAULT_FORECAST_DIR_BG = Path("./miner_forecasts/")
MINER_FORECAST_DIR_BG = Path(os.getenv("MINER_FORECAST_DIR", DEFAULT_FORECAST_DIR_BG))
VALIDATOR_ENSEMBLE_DIR = Path("./validator_ensembles/")
MINER_FORECAST_DIR_BG.mkdir(parents=True, exist_ok=True)
VALIDATOR_ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)

def _load_config(self):
    """Loads configuration for the WeatherTask from environment variables."""
    logger.info("Loading WeatherTask configuration from environment variables...")
    config = {}
    
    def parse_int_list(env_var, default_list): 
        val_str = os.getenv(env_var)
        if val_str:
            try: return [int(x.strip()) for x in val_str.split(',')]
            except ValueError: logger.warning(f"Invalid format for {env_var}: '{val_str}'. Using default.")
        return default_list
        
    # Worker/Run Parameters
    config['max_concurrent_inferences'] = int(os.getenv('WEATHER_MAX_CONCURRENT_INFERENCES', '1'))
    config['inference_steps'] = int(os.getenv('WEATHER_INFERENCE_STEPS', '40'))
    config['forecast_step_hours'] = int(os.getenv('WEATHER_FORECAST_STEP_HOURS', '6'))
    config['forecast_duration_hours'] = config['inference_steps'] * config['forecast_step_hours']
    
    # Scoring Parameters
    config['initial_scoring_lead_hours'] = parse_int_list('WEATHER_INITIAL_SCORING_LEAD_HOURS', [6, 12]) # Day 0.25, 0.5
    config['final_scoring_lead_hours'] = parse_int_list('WEATHER_FINAL_SCORING_LEAD_HOURS', [60,78,138]) # Day 2, 3, 5
    config['verification_wait_minutes'] = int(os.getenv('WEATHER_VERIFICATION_WAIT_MINUTES', '60'))
    config['verification_timeout_seconds'] = int(os.getenv('WEATHER_VERIFICATION_TIMEOUT_SECONDS', '3600'))
    config['final_scoring_check_interval_seconds'] = int(os.getenv('WEATHER_FINAL_SCORING_INTERVAL_S', '3600'))
    config['era5_delay_days'] = int(os.getenv('WEATHER_ERA5_DELAY_DAYS', '5'))
    config['era5_buffer_hours'] = int(os.getenv('WEATHER_ERA5_BUFFER_HOURS', '6'))
    config['cleanup_check_interval_seconds'] = int(os.getenv('WEATHER_CLEANUP_INTERVAL_S', '21600')) # 6 hours
    config['gfs_analysis_cache_dir'] = os.getenv('WEATHER_GFS_CACHE_DIR', './gfs_analysis_cache')
    config['era5_cache_dir'] = os.getenv('WEATHER_ERA5_CACHE_DIR', './era5_cache')
    config['gfs_cache_retention_days'] = int(os.getenv('WEATHER_GFS_CACHE_RETENTION_DAYS', '7'))
    config['era5_cache_retention_days'] = int(os.getenv('WEATHER_ERA5_CACHE_RETENTION_DAYS', '30'))
    config['ensemble_retention_days'] = int(os.getenv('WEATHER_ENSEMBLE_RETENTION_DAYS', '14'))
    config['db_run_retention_days'] = int(os.getenv('WEATHER_DB_RUN_RETENTION_DAYS', '90'))
    config['run_hour_utc'] = int(os.getenv('WEATHER_RUN_HOUR_UTC', '18'))
    config['run_minute_utc'] = int(os.getenv('WEATHER_RUN_MINUTE_UTC', '0'))
    config['validator_hash_wait_minutes'] = int(os.getenv('WEATHER_VALIDATOR_HASH_WAIT_MINUTES', '10'))

    # --- R2 Cleanup Config (Credentials are handled by the Miner, not here) ---
    config['r2_cleanup_enabled'] = os.getenv('WEATHER_R2_CLEANUP_ENABLED', 'true').lower() in ['true', '1', 'yes']
    config['r2_cleanup_interval_seconds'] = int(os.getenv('WEATHER_R2_CLEANUP_INTERVAL_S', '3600')) # 1 hour
    config['r2_max_inputs_to_keep'] = int(os.getenv('WEATHER_R2_MAX_INPUTS_TO_KEEP', '5'))
    config['r2_max_forecasts_to_keep'] = int(os.getenv('WEATHER_R2_MAX_FORECASTS_TO_KEEP', '5'))


    config['miner_jwt_secret_key'] = os.getenv("MINER_JWT_SECRET_KEY", "insecure_default_key_for_development_only")
    config['jwt_algorithm'] = os.getenv("MINER_JWT_ALGORITHM", "HS256")
    config['access_token_expire_minutes'] = int(os.getenv('WEATHER_ACCESS_TOKEN_EXPIRE_MINUTES', '120'))

    config['era5_climatology_path'] = os.getenv(
        'WEATHER_ERA5_CLIMATOLOGY_PATH', 
        'gs://weatherbench2/datasets/era5-hourly-climatology/1990-2019_6h_1440x721.zarr'
    )

    # Day-1 Scoring Specific Configurations
    default_day1_vars_levels = [
        {"name": "z", "level": 500, "standard_name": "geopotential"},
        {"name": "t", "level": 850, "standard_name": "temperature"},
        {"name": "2t", "level": None, "standard_name": "2m_temperature"},
        {"name": "msl", "level": None, "standard_name": "mean_sea_level_pressure"}
    ]
    try:
        day1_vars_levels_json = os.getenv('WEATHER_DAY1_VARIABLES_LEVELS_JSON')
        config['day1_variables_levels_to_score'] = json.loads(day1_vars_levels_json) if day1_vars_levels_json else default_day1_vars_levels
    except json.JSONDecodeError:
        logger.warning("Invalid JSON for WEATHER_DAY1_VARIABLES_LEVELS_JSON. Using default.")
        config['day1_variables_levels_to_score'] = default_day1_vars_levels

    default_day1_clim_bounds = {
        "2t": (180, 340),                             # Kelvin
        "msl": (90000, 110000),                       # Pascals
        "t850": (220, 320),                           # Kelvin for 850hPa Temperature
        "z500": (45000, 60000)                        # m^2/s^2 for Geopotential at 500hPa (approx 4500-6000 gpm)
    }
    try:
        day1_clim_bounds_json = os.getenv('WEATHER_DAY1_CLIMATOLOGY_BOUNDS_JSON')
        config['day1_climatology_bounds'] = json.loads(day1_clim_bounds_json) if day1_clim_bounds_json else default_day1_clim_bounds
    except json.JSONDecodeError:
        logger.warning("Invalid JSON for WEATHER_DAY1_CLIMATOLOGY_BOUNDS_JSON. Using default.")
        config['day1_climatology_bounds'] = default_day1_clim_bounds

    config['day1_pattern_correlation_threshold'] = float(os.getenv('WEATHER_DAY1_PATTERN_CORR_THRESHOLD', '0.3'))
    config['day1_acc_lower_bound'] = float(os.getenv('WEATHER_DAY1_ACC_LOWER_BOUND', '0.6'))
    config['day1_alpha_skill'] = float(os.getenv('WEATHER_DAY1_ALPHA_SKILL', '0.6'))
    config['day1_beta_acc'] = float(os.getenv('WEATHER_DAY1_BETA_ACC', '0.4'))

    # Quality Control Config - I need to tune these values
    config['day1_clone_penalty_gamma'] = float(os.getenv('WEATHER_DAY1_CLONE_PENALTY_GAMMA', '1.0'))
    default_clone_delta_thresholds = {
        "2t": 0.01,  # (RMSE 0.1K)^2
        "msl": 20000, # (RMSE 100Pa or 1hPa)^2
        "z500": 10000,   # (RMSE 100 m^2/s^2)^2 for geopotential
        "t850": 0.25    # (RMSE 0.5K)^2 
    }
    try:
        clone_delta_json = os.getenv('WEATHER_DAY1_CLONE_DELTA_THRESHOLDS_JSON')
        config['day1_clone_delta_thresholds'] = json.loads(clone_delta_json) if clone_delta_json else default_clone_delta_thresholds
    except json.JSONDecodeError:
        logger.warning("Invalid JSON for WEATHER_DAY1_CLONE_DELTA_THRESHOLDS_JSON. Using default.")
        config['day1_clone_delta_thresholds'] = default_clone_delta_thresholds

    config['weather_score_day1_weight'] = float(os.getenv('WEATHER_SCORE_DAY1_WEIGHT', '0.2'))
    config['weather_score_era5_weight'] = float(os.getenv('WEATHER_SCORE_ERA5_WEIGHT', '0.8'))
    config['weather_bonus_value_add'] = float(os.getenv('WEATHER_BONUS_VALUE_ADD', '0.05')) # Value to add for winning a bonus category

    # --- RunPod Specific Config ---
    config['runpod_poll_interval_seconds'] = int(os.getenv('RUNPOD_POLL_INTERVAL_SECONDS', '10'))
    config['runpod_max_poll_attempts'] = int(os.getenv('RUNPOD_MAX_POLL_ATTEMPTS', '900')) # 900 attempts * 10s/attempt = 9000s = 150 minutes = 2.5 hours
    config['runpod_download_endpoint_suffix'] = os.getenv('RUNPOD_DOWNLOAD_ENDPOINT_SUFFIX', 'run/download_step') # e.g., "run/download_step"
    config['runpod_upload_input_suffix'] = os.getenv('RUNPOD_UPLOAD_INPUT_SUFFIX', 'run/upload_input') # e.g., "run/upload_input"

     # Default to 6 hours

    logger.info(f"WeatherTask configuration loaded: {config}")
    return config

class WeatherTask(Task):
    db_manager: Union[ValidatorDatabaseManager, MinerDatabaseManager]
    node_type: str = Field(default="validator")
    test_mode: bool = Field(default=False)
    era5_climatology_ds: Optional[xr.Dataset] = Field(default=None, exclude=True)
 
    model_config = ConfigDict(
        extra = 'allow',
        arbitrary_types_allowed=True
    )

    def __init__(self, db_manager=None, node_type=None, test_mode=False, keypair=None, 
                 inference_service_url: Optional[str] = None, 
                 runpod_api_key: Optional[str] = None, # For RunPod API Key (if CREDENTIAL was set)
                 r2_config: Optional[Dict[str, str]] = None,
                 **data):
        loaded_config = self._load_config() 
        
        # Merge keyword arguments into the loaded config, allowing overrides
        loaded_config.update(data)

        super_data = {
            "name": "WeatherTask",
            "description": "Weather forecast generation and verification task",
            "task_type": "atomic",
            "metadata": WeatherMetadata(),
            "db_manager": db_manager,
            "node_type": node_type,
            "test_mode": test_mode,
            **data 
        }
        super().__init__(**super_data)
        
        self.keypair = keypair
        self.db_manager = db_manager
        self.node_type = node_type
        self.test_mode = test_mode
        self.config = loaded_config 
        self.r2_config = r2_config # Store the dedicated R2 config
        self.validator = data.get('validator')
        self.era5_climatology_ds = None
        self.inference_service_url = inference_service_url # Store the URL
        self.runpod_api_key = runpod_api_key # Store RunPod API key (will be None if not provided)
        logger.info(f"WeatherTask initialized. RunPod API Key is {'SET' if self.runpod_api_key else 'NOT SET'}. First 5 chars: {self.runpod_api_key[:5] if self.runpod_api_key else 'N/A'}")

        self.initial_scoring_queue = asyncio.Queue()
        self.initial_scoring_worker_running = False
        self.initial_scoring_workers = []
        self.final_scoring_worker_running = False
        self.final_scoring_workers = []
        self.cleanup_worker_running = False
        self.cleanup_workers = []
        self.r2_cleanup_worker_running = False # For R2 cleanup
        self.r2_cleanup_workers = [] # For R2 cleanup
        
        self.test_mode_run_scored_event = asyncio.Event()
        self.last_test_mode_run_id = None
        
        self.gpu_semaphore = asyncio.Semaphore(self.config.get('max_concurrent_inferences', 1))
        
        era5_scoring_concurrency = int(os.getenv('WEATHER_VALIDATOR_ERA5_SCORING_CONCURRENCY', '4'))
        self.era5_scoring_semaphore = asyncio.Semaphore(era5_scoring_concurrency)
        logger.info(f"ERA5 scoring concurrency for validator set to: {era5_scoring_concurrency}")

        self.inference_runner = None 
        if self.node_type == "miner":
            try:
                # Inference type from .env or defaults
                self.config['weather_inference_type'] = os.getenv("WEATHER_INFERENCE_TYPE", "local_model").lower()
                load_local_model_flag = True

                if self.config['weather_inference_type'] == "http_service":
                    logger.info(f"WeatherTask.__init__: Configured to use HTTP Inference Service. Provided URL: '{self.inference_service_url}'") # Log the provided URL
                    if not self.inference_service_url:
                        logger.error("WeatherTask.__init__: HTTP Inference Service selected, but no valid URL provided to WeatherTask constructor. Cannot use HTTP inference.")
                        # Fallback or error needed? For now, it will try to load local if URL is missing.
                        # To prevent local model loading if HTTP was intended but URL is missing:
                        load_local_model_flag = True # Fallback to trying local model if URL is bad.
                        self.config['weather_inference_type'] = "local_model" # Explicitly set to local_model
                        logger.warning("WeatherTask.__init__: Falling back to 'local_model' due to missing/invalid HTTP service URL.")
                    else:
                        logger.info("WeatherTask.__init__: HTTP Inference Service URL is present. Will proceed with HTTP client logic. Local model will NOT be loaded.")
                        load_local_model_flag = False # Don't load local model if using HTTP service

                elif self.config['weather_inference_type'] == "azure_foundry":
                    logger.info("Configured to use Azure Foundry for inference. Local model will not be loaded.")
                    load_local_model_flag = False
                elif self.config['weather_inference_type'] == "local_model":
                    logger.info("Configured to use local model for inference.")
                else:
                    logger.warning(f"Invalid WEATHER_INFERENCE_TYPE: '{self.config['weather_inference_type']}'. Defaulting to 'local_model'.")
                    self.config['weather_inference_type'] = "local_model"
                
                device = "cuda" if torch.cuda.is_available() else "cpu"
                if load_local_model_flag:
                    self.inference_runner = WeatherInferenceRunner(device=device, load_local_model=True)
                    if self.inference_runner.model is not None:
                        logger.info(f"Initialized Miner components (Local Inference Runner on {device}, model loaded).")
                    else:
                        logger.error(f"Initialized Miner components (Local Inference Runner on {device}, BUT MODEL FAILED TO LOAD).")
                elif self.config['weather_inference_type'] == "http_service" and self.inference_service_url:
                    logger.info(f"Miner configured for HTTP inference. Local model not loaded. Inference runner will be None.")
                    self.inference_runner = None # Explicitly None if using HTTP service
                else:
                    logger.info(f"Miner components initialized. Local model not loaded (inference type: {self.config['weather_inference_type']}). Inference runner will be None.")
                    self.inference_runner = None # Explicitly None if local model is not to be loaded

            except Exception as e:
                logger.error(f"Failed to initialize WeatherInferenceRunner or set inference type: {e}", exc_info=True)
                self.inference_runner = None 
                self.config['weather_inference_type'] = "local" # Fallback
        else: # Validator
             logger.info("Initialized validator components for WeatherTask")

    _load_config = _load_config

    ############################################################
    # RunPodCTL Helpers (Synchronous, for WeatherTask execution path)
    ############################################################
    def _run_weather_task_subprocess_command(
        self, 
        command_parts: List[str], 
        timeout_seconds: int = 300,
        extract_code_and_terminate: bool = False,
        extract_code_prefix: str = "Code is: "
    ) -> Tuple[bool, str, str]:
        """
        Runs a subprocess command.
        If extract_code_and_terminate is True, it reads stdout line by line for a specific prefix,
        extracts the value, terminates the process, and returns the code.
        Otherwise, it waits for the command to complete or timeout using communicate().
        Returns a tuple: (success_boolean, stdout_str, stderr_str).
        """
        try:
            full_command = ' '.join(command_parts)
            logger.info(f"Executing command: {full_command} with timeout {timeout_seconds}s. Extract and terminate: {extract_code_and_terminate}")
            
            current_env = os.environ.copy()
            logger.debug(f"Subprocess PATH: {current_env.get('PATH', 'N/A')}")

            process = subprocess.Popen(
                command_parts, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                text=True, 
                bufsize=1, 
                universal_newlines=True,
                env=current_env
            )
            
            if extract_code_and_terminate:
                stdout_lines = []
                stderr_lines = [] 
                extracted_code = ""
                code_found = False
                
                start_time = time.time()
                while time.time() - start_time < timeout_seconds:
                    if process.poll() is not None:
                        logger.warning(f"Process '{full_command}' terminated prematurely with code {process.returncode} while waiting for code line.")
                        break
                    
                    if process.stdout:
                        line = process.stdout.readline()
                        if line:
                            stdout_lines.append(line)
                            # MODIFIED CONDITION: If the stripped line from stdout is non-empty, consider it the code.
                            line_stripped = line.strip()
                            if line_stripped: 
                                extracted_code = line_stripped
                                code_found = True
                                logger.info(f"Extracted code from stdout: '{extracted_code}' (from raw line: '{line.strip()}')")
                                break 
                        else: 
                            logger.debug(f"End of stdout stream reached for '{full_command}' before code found or timeout.")
                            break 
                    time.sleep(0.01)

                if code_found:
                    logger.info(f"Code '{extracted_code}' extracted. Terminating process '{full_command}'.")
                    process.terminate()
                    try:
                        remaining_stdout, final_stderr = process.communicate(timeout=5) 
                        if remaining_stdout:
                            stdout_lines.append(remaining_stdout)
                        if final_stderr:
                            stderr_lines.append(final_stderr)
                    except subprocess.TimeoutExpired:
                        logger.warning(f"Timeout waiting for process '{full_command}' to terminate/communicate after code extraction.")
                        process.kill()
                        try:
                            remaining_stdout_kill, final_stderr_kill = process.communicate()
                            if remaining_stdout_kill:
                                stdout_lines.append(remaining_stdout_kill)
                            if final_stderr_kill:
                                stderr_lines.append(final_stderr_kill)
                        except Exception as e_kill_comm:
                            logger.warning(f"Exception during final communicate after kill for '{full_command}': {e_kill_comm}")
                    except Exception as e_comm:
                        logger.warning(f"Exception during final communicate for '{full_command}' after code extraction: {e_comm}")
                    
                    full_stdout_str = "".join(stdout_lines).strip()
                    full_stderr_str = "".join(stderr_lines).strip()
                    logger.debug(f"Command '{full_command}' (extract mode) successful. Code: {extracted_code}. Full Stdout (first 500 chars): {full_stdout_str[:500]}. Full Stderr (first 500 chars): {full_stderr_str[:500]}.")
                    # Verify if "Code is: <extracted_code>" is in stderr for confirmation
                    # This uses the extract_code_prefix passed to the function.
                    if f"{extract_code_prefix}{extracted_code}" in full_stderr_str:
                        logger.info(f"Confirmation: Extracted code '{extracted_code}' found in stderr output with prefix '{extract_code_prefix}'.")
                    else:
                        logger.warning(f"Verification: Extracted code '{extracted_code}' from stdout NOT found in stderr output (stderr: '{full_stderr_str[:200]}...') with prefix '{extract_code_prefix}'. This might be okay if stdout is the sole reliable source for the code itself.")
                    
                    full_stdout_log = "".join(stdout_lines).strip()
                    return True, extracted_code, full_stderr_str 
                else: 
                    if process.poll() is None:
                        process.kill()
                    try:
                        remaining_stdout, remaining_stderr = process.communicate(timeout=5) 
                        if remaining_stdout:
                            stdout_lines.append(remaining_stdout)
                        if remaining_stderr:
                            stderr_lines.append(remaining_stderr)
                    except Exception: 
                        pass 
                    
                    full_stdout_str = "".join(stdout_lines).strip()
                    full_stderr_str = "".join(stderr_lines).strip()
                    logger.error(f"Command '{full_command}' (extract mode) failed: Code '{extract_code_prefix}...' not found or timeout. Full Stdout (first 500 chars): {full_stdout_str[:500]}. Full Stderr (first 500 chars): {full_stderr_str[:500]}.")
                    return False, full_stdout_str, f"Code not found or timeout. Partial stderr: {full_stderr_str}"

            else: # Original behavior: wait for process to complete
                try:
                    stdout, stderr = process.communicate(timeout=timeout_seconds)
                except subprocess.TimeoutExpired:
                    process.kill() 
                    stdout_on_timeout, stderr_on_timeout = process.communicate() 
                    logger.error(f"Command '{full_command}' timed out after {timeout_seconds} seconds.")
                    s_out = stdout_on_timeout.strip() if stdout_on_timeout else ""
                    s_err = stderr_on_timeout.strip() if stderr_on_timeout else ""
                    logger.debug(f"Stdout on timeout (first 500 chars): {s_out[:500]}")
                    logger.debug(f"Stderr on timeout (first 500 chars): {s_err[:500]}")
                    return False, s_out, f"Command timed out. Partial stderr: {s_err}"

                max_log_len = 1000 
                stripped_stdout = stdout.strip() if stdout else ""
                stripped_stderr = stderr.strip() if stderr else ""

                logger.debug(f"Command '{full_command}' completed. Return code: {process.returncode}")
                logger.debug(f"Stdout (len: {len(stripped_stdout)}): {stripped_stdout[:max_log_len]}{'...' if len(stripped_stdout) > max_log_len else ''}")
                logger.debug(f"Stderr (len: {len(stripped_stderr)}): {stripped_stderr[:max_log_len]}{'...' if len(stripped_stderr) > max_log_len else ''}")

                if process.returncode == 0:
                    logger.info(f"Command '{full_command}' successful.")
                    return True, stripped_stdout, stripped_stderr
                else:
                    logger.error(f"Command '{full_command}' failed with return code {process.returncode}.")
                    return False, stripped_stdout, stripped_stderr
                
        except FileNotFoundError:
            logger.error(f"Command not found: {command_parts[0]}. Ensure it is installed and in PATH.")
            return False, "", f"Command not found: {command_parts[0]}"
        except Exception as e:
            # Corrected f-string for the exception logging
            logger.error(f"An error occurred while running command '{' '.join(command_parts)}': {e}", exc_info=True)
            return False, "", str(e)

    async def _get_r2_s3_client(self) -> Optional[boto3.client]:
        """
        Initializes and returns a boto3 S3 client configured for R2.
        Credentials are now sourced from self.r2_config, which is provided
        by the Miner during instantiation.
        Returns None if configuration is missing.
        """
        if not self.r2_config:
            logger.error("R2 client configuration (self.r2_config) not provided to WeatherTask. Cannot create S3 client.")
            return None

        endpoint_url = self.r2_config.get("r2_endpoint_url")
        access_key_id = self.r2_config.get("r2_access_key_id")
        secret_access_key = self.r2_config.get("r2_secret_access_key")
        bucket_name = self.r2_config.get("r2_bucket_name")

        if not all([endpoint_url, access_key_id, secret_access_key, bucket_name]):
            logger.error("R2 client configuration is incomplete in self.r2_config. Please ensure R2_ENDPOINT_URL, R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, and R2_BUCKET_NAME are set in the environment for the miner.")
            return None
        
        try:
            s3_client = boto3.client(
                's3',
                endpoint_url=endpoint_url,
                aws_access_key_id=access_key_id,
                aws_secret_access_key=secret_access_key,
                config=boto3.session.Config(signature_version='s3v4'),
                region_name='auto' # R2 is region-less
            )
            return s3_client
        except Exception as e:
            logger.error(f"Failed to create R2 S3 client: {e}", exc_info=True)
            return None

    async def _upload_input_to_r2(self, s3_client: boto3.client, job_id: str, initial_batch: 'Batch') -> Optional[str]:
        """Uploads the initial batch pickle to R2."""
        if not self.r2_config or not self.r2_config.get("r2_bucket_name"):
            logger.error(f"[{job_id}] R2 bucket name not found in self.r2_config. Cannot upload input.")
            return None
        bucket_name = self.r2_config.get("r2_bucket_name")

        object_key = f"inputs/{job_id}/initial_batch.pkl"
        
        try:
            with io.BytesIO() as f:
                pickle.dump(initial_batch, f)
                f.seek(0)
                await asyncio.to_thread(
                    s3_client.upload_fileobj,
                    f,
                    bucket_name,
                    object_key
                )
            logger.info(f"[{job_id}] Successfully uploaded initial batch to R2: s3://{bucket_name}/{object_key}")
            return object_key
        except (BotoClientError, Exception) as e:
            logger.error(f"[{job_id}] Failed to upload initial batch to R2: {e}", exc_info=True)
            return None

    async def _invoke_inference_service(self, job_id: str, input_r2_key: str) -> Optional[str]:
        """
        Invokes the asynchronous inference service (e.g., RunPod) to START a job.
        Returns the runpod_job_id if successful, otherwise None. Does NOT poll.
        """
        if not self.inference_service_url or not self.runpod_api_key:
            logger.error(f"[{job_id}] Inference service URL or API key is not configured.")
            return None

        headers = {"Authorization": f"Bearer {self.runpod_api_key}"}
        
        # Ensure the URL is for the 'run' endpoint
        if not self.inference_service_url.endswith(('/run', '/run/')):
            logger.error(f"[{job_id}] Inference URL is not a 'run' endpoint: {self.inference_service_url}")
            return None
        
        payload = {
            "input": {
                "action": "run_inference_from_r2",
                "input_r2_object_key": input_r2_key,
                "job_run_uuid": job_id
            }
        }

        async with httpx.AsyncClient(headers=headers, timeout=30.0) as client:
            try:
                logger.info(f"[{job_id}] Sending inference start request to {self.inference_service_url}")
                response = await client.post(self.inference_service_url, json=payload)
                response.raise_for_status()
                
                start_data = response.json()
                runpod_job_id = start_data.get("id")

                if not runpod_job_id:
                    logger.error(f"[{job_id}] Service did not return a RunPod job ID. Response: {start_data}")
                    return None
                
                logger.info(f"[{job_id}] Job started on RunPod. RunPod Job ID: {runpod_job_id}")
                return runpod_job_id

            except (httpx.HTTPStatusError, httpx.RequestError, json.JSONDecodeError) as e:
                logger.error(f"[{job_id}] Error starting inference job: {e}", exc_info=True)
                return None

    async def _run_inference_via_http_service(self, job_id: str) -> bool:
        """
        Orchestrates running inference via the HTTP service.
        This is now fully asynchronous and returns immediately after launching the job.
        """
        logger.info(f"[{job_id}] Starting async inference via HTTP service.")
        s3_client = await self._get_r2_s3_client()
        if not s3_client:
            await update_job_status(self, job_id, 'failed', "R2 client configuration invalid.")
            return False

        initial_batch = await self._load_batch_from_db(job_id)
        if not initial_batch:
            await update_job_status(self, job_id, 'failed', "Failed to load initial data batch.")
            return False

        input_r2_key = await self._upload_input_to_r2(s3_client, job_id, initial_batch)
        if not input_r2_key:
            await update_job_status(self, job_id, 'failed', "Failed to upload input to R2.")
            return False

        runpod_job_id = await self._invoke_inference_service(job_id, input_r2_key)
        if not runpod_job_id:
            await update_job_status(self, job_id, 'failed', "Failed to start job on RunPod.")
            return False

        # Save the RunPod job ID and launch the poller
        try:
            query = "UPDATE weather_miner_jobs SET runpod_job_id = :runpod_id WHERE id = :job_id"
            await self.db_manager.execute(query, {"runpod_id": runpod_job_id, "job_id": job_id})
            
            logger.info(f"[{job_id}] Saved RunPod ID {runpod_job_id}. Launching background poller.")
            asyncio.create_task(poll_runpod_job_worker(self, job_id, runpod_job_id))
            return True
        except Exception as e:
            error_msg = f"DB error after starting RunPod job: {e}"
            logger.error(f"[{job_id}] {error_msg}", exc_info=True)
            await update_job_status(self, job_id, 'failed', error_msg)
            # Should also attempt to cancel the runpod job here if possible
            return False

    ############################################################
    # Validator methods
    ############################################################

    async def _get_or_load_era5_climatology(self) -> Optional[xr.Dataset]:
        if self.era5_climatology_ds is None:
            climatology_path = self.config.get("era5_climatology_path")
            if climatology_path:
                try:
                    logger.info(f"Loading ERA5 climatology from: {climatology_path}")
                    self.era5_climatology_ds = await asyncio.to_thread(
                        xr.open_zarr, climatology_path, consolidated=True 
                    )
                    logger.info("ERA5 climatology loaded successfully.")
                except Exception as e:
                    logger.error(f"Failed to load ERA5 climatology from {climatology_path}: {e}", exc_info=True)
                    self.era5_climatology_ds = None
            else:
                logger.error("WEATHER_ERA5_CLIMATOLOGY_PATH not configured. Cannot load climatology for ACC calculation.")
        return self.era5_climatology_ds

    async def build_score_row(self, run_id: int, gfs_init_time: datetime, evaluation_results: List[Dict], task_name_prefix: str):
        """
        Builds the score row for a given run and stores it in the score_table.
        evaluation_results is a list of dicts, each from an evaluation function (e.g., evaluate_miner_forecast_day1).
        task_name_prefix is used to form the task_name in score_table (e.g., 'weather_day1_qc', 'weather_era5_final').
        """
        logger.info(f"[BuildScoreRow] Building {task_name_prefix} score row for run_id: {run_id}")
        all_miner_scores_for_run: Dict[int, float] = {}

        for eval_result in evaluation_results:
            if isinstance(eval_result, Exception) or not isinstance(eval_result, dict):
                logger.warning(f"[BuildScoreRow] Skipping invalid evaluation result for {task_name_prefix}: {type(eval_result)}")
                continue
            
            miner_uid = eval_result.get("miner_uid")
            score_value = eval_result.get("final_score_for_uid") 

            if miner_uid is not None and score_value is not None and np.isfinite(score_value):
                all_miner_scores_for_run[miner_uid] = float(score_value)
            elif miner_uid is not None:
                all_miner_scores_for_run[miner_uid] = 0.0 

        final_scores_list = [0.0] * 256
        
        for uid, score in all_miner_scores_for_run.items():
            if 0 <= uid < 256:
                final_scores_list[uid] = score
        

        score_row_data = {
            "task_name": task_name_prefix,
            "task_id": str(run_id),
            "score": final_scores_list,
            "status": f"{task_name_prefix}_scores_compiled",
            "gfs_init_time_for_table": gfs_init_time
        }

        try:
            upsert_score_table_query = """
                INSERT INTO score_table (task_name, task_id, score, status, created_at)
                VALUES (:task_name, :task_id, :score, :status, :created_at_val)
                ON CONFLICT (task_name, task_id) DO UPDATE SET
                    score = EXCLUDED.score,
                    status = EXCLUDED.status,
                    created_at = EXCLUDED.created_at
            """
            
            db_params_score_table = {
                "task_name": score_row_data["task_name"],
                "task_id": score_row_data["task_id"],
                "score": score_row_data["score"],
                "status": score_row_data["status"],
                "created_at_val": score_row_data["gfs_init_time_for_table"]
            }

            await self.db_manager.execute(upsert_score_table_query, db_params_score_table)
            logger.info(f"[BuildScoreRow] Upserted score_table entry for {task_name_prefix}, task_id (run_id): {run_id}")

        except Exception as e_db_score_table:
            logger.error(f"[BuildScoreRow] DB error storing {task_name_prefix} score row for run {run_id}: {e_db_score_table}", exc_info=True)

    async def validator_prepare_subtasks(self):
        """
        Prepares data needed for a forecast run (e.g., identifying GFS data).
        Since this is 'atomic', it doesn't prepare sub-tasks in the composite sense,
        but rather the overall input for querying miners.
        """
        pass

    async def validator_execute(self, validator):
        """
        Orchestrates the weather forecast task for the validator:
        1. Waits for the scheduled run time (e.g., daily post-00Z GFS availability).
        2. Fetches necessary GFS analysis data (T=0h from 00Z run, T=-6h from previous 18Z run).
        3. Serializes data and creates a run record in DB.
        4. Queries miners with the payload (/weather-forecast-request).
        5. Records miner acceptances in DB.
        """
        self.validator = validator
        logger.info("Starting WeatherTask validator execution loop...")
        
        if not self.initial_scoring_worker_running:
            await self.start_background_workers(num_initial_scoring_workers=1, num_final_scoring_workers=1, num_cleanup_workers=1)
            logger.info("Started background workers for scoring and cleanup.")

        run_hour_utc = self.config.get('run_hour_utc', 18)
        run_minute_utc = self.config.get('run_minute_utc', 0)
        logger.info(f"Validator execute loop configured to run around {run_hour_utc:02d}:{run_minute_utc:02d} UTC.")

        if self.test_mode:
             logger.warning("Running in TEST MODE: Execution will run once immediately.")

        while True:
            try:
                await validator.update_task_status('weather', 'active', 'waiting')
                now_utc = datetime.now(timezone.utc)

                if not self.test_mode:
                    target_run_time_today = now_utc.replace(hour=run_hour_utc, minute=run_minute_utc, second=0, microsecond=0)
                    if now_utc >= target_run_time_today:
                        next_run_trigger_time = target_run_time_today + timedelta(days=1)
                    else:
                         next_run_trigger_time = target_run_time_today

                    wait_seconds = (next_run_trigger_time - now_utc).total_seconds()
                    logger.info(f"Current time: {now_utc}. Next weather run scheduled at {next_run_trigger_time}. Waiting for {wait_seconds:.2f} seconds.")
                    if wait_seconds > 0:
                         await asyncio.sleep(wait_seconds)
                    now_utc = datetime.now(timezone.utc)

                logger.info(f"Initiating weather forecast run triggered around {now_utc}...")
                await validator.update_task_status('weather', 'processing', 'initializing_run')

                if self.test_mode:
                    logger.info(f"Test mode enabled. Adjusting GFS init time by -1 day from {now_utc.strftime('%Y-%m-%d %H:%M')} UTC.")
                    now_utc -= timedelta(days=1)
                    logger.info(f"Adjusted GFS init time for test mode: {now_utc.strftime('%Y-%m-%d %H:%M')} UTC.")
                    
                gfs_t0_run_time = now_utc.replace(hour=0, minute=0, second=0, microsecond=0)
                gfs_t_minus_6_run_time = gfs_t0_run_time - timedelta(hours=6) # This will be 18Z from the previous day

                logger.info(f"Target GFS T=0h Analysis Run Time: {gfs_t0_run_time}")
                logger.info(f"Target GFS T=-6h Analysis Run Time: {gfs_t_minus_6_run_time}")

                run_id = None

                try:
                    run_insert_query = """
                        INSERT INTO weather_forecast_runs (run_initiation_time, target_forecast_time_utc, gfs_init_time_utc, status)
                        VALUES (:init_time, :target_time, :gfs_init, :status)
                        RETURNING id
                    """
                    effective_forecast_start_time = gfs_t0_run_time
                    run_record = await self.db_manager.fetch_one(run_insert_query, { 
                         "init_time": now_utc,
                         "target_time": effective_forecast_start_time,
                         "gfs_init": effective_forecast_start_time,
                         "status": "fetching_gfs"
                    })

                    if run_record and 'id' in run_record:
                        run_id = run_record['id']
                        logger.info(f"[Run {run_id}] Created weather_forecast_runs record with ID: {run_id}")
                    else:
                        logger.error("Failed to retrieve run_id using fetch_one after insert.")
                        raise RuntimeError("Failed to create run_id for forecast run.")

                except Exception as db_err:
                     logger.error(f"Failed to create forecast run record in DB: {db_err}", exc_info=True)
                     if self.test_mode:
                         logger.warning("TEST MODE: DB error during run_id creation. Exiting validator_execute loop.")
                         break 
                     else:
                         await asyncio.sleep(60)
                         continue 

                await validator.update_task_status('weather', 'processing', 'sending_fetch_requests')
                await _update_run_status(self, run_id, "sending_fetch_requests")

                payload_data = WeatherInitiateFetchData(
                     forecast_start_time=gfs_t0_run_time,   # T=0h time
                     previous_step_time=gfs_t_minus_6_run_time # T=-6h time
                )
                payload_dict = payload_data.model_dump(mode='json')
                
                payload = {
                     "nonce": str(uuid.uuid4()),
                     "data": payload_dict 
                }

                logger.info(f"[Run {run_id}] Querying miners with weather initiate fetch request (Endpoint: /weather-initiate-fetch)... Payload size approx: {len(json.dumps(payload))} bytes")
                responses = await validator.query_miners(
                     payload=payload,
                     endpoint="/weather-initiate-fetch"
                )
                logger.info(f"[Run {run_id}] Received {len(responses)} initial responses from miners for fetch initiation.")

                await validator.update_task_status('weather', 'processing', 'recording_acceptances')
                accepted_count = 0
                for miner_hotkey, response_data in responses.items():
                     try:
                         miner_response = response_data
                         if isinstance(response_data, dict) and 'text' in response_data:
                             try:
                                 miner_response = json.loads(response_data['text'])
                                 logger.debug(f"[Run {run_id}] Parsed JSON from response text for {miner_hotkey}: {miner_response}")
                             except (json.JSONDecodeError, TypeError) as json_err:
                                 logger.warning(f"[Run {run_id}] Failed to parse response text for {miner_hotkey}: {json_err}")
                                 miner_response = {"status": "parse_error", "message": str(json_err)}
                                 
                         if (isinstance(miner_response, dict) and 
                             miner_response.get("status") == "fetch_accepted" and 
                             miner_response.get("job_id")):
                              miner_uid_result = await self.db_manager.fetch_one("SELECT uid FROM node_table WHERE hotkey = :hk", {"hk": miner_hotkey})
                              miner_uid = miner_uid_result['uid'] if miner_uid_result else -1
 
                              if miner_uid == -1:
                                  logger.warning(f"[Run {run_id}] Miner {miner_hotkey} accepted but UID not found in node_table.")
                                  continue
 
                              miner_job_id = miner_response.get("job_id")
 
                              insert_resp_query = """
                                   INSERT INTO weather_miner_responses
                                    (run_id, miner_uid, miner_hotkey, response_time, status, job_id)
                                    VALUES (:run_id, :uid, :hk, :resp_time, :status, :job_id)
                                   ON CONFLICT (run_id, miner_uid) DO UPDATE SET
                                    response_time = EXCLUDED.response_time, 
                                    status = EXCLUDED.status,
                                    job_id = EXCLUDED.job_id
                              """
                              await self.db_manager.execute(insert_resp_query, {
                                   "run_id": run_id,
                                   "uid": miner_uid,
                                   "hk": miner_hotkey,
                                   "resp_time": datetime.now(timezone.utc),
                                   "status": "fetch_initiated",
                                   "job_id": miner_job_id
                              })
                              accepted_count += 1
                              logger.debug(f"[Run {run_id}] Recorded acceptance from Miner UID {miner_uid} ({miner_hotkey}). Miner Job ID: {miner_job_id}")
                         else:
                              logger.warning(f"[Run {run_id}] Miner {miner_hotkey} did not return successful 'fetch_accepted' status or job_id. Response: {miner_response}")
                     except Exception as resp_proc_err:
                          logger.error(f"[Run {run_id}] Error processing response from {miner_hotkey}: {resp_proc_err}", exc_info=True)

                logger.info(f"[Run {run_id}] Completed processing initiate fetch responses. {accepted_count} miners accepted.")
                await _update_run_status(self, run_id, "awaiting_input_hashes") 

                wait_minutes = self.config.get('validator_hash_wait_minutes', 10)
                if self.test_mode:
                    original_wait = wait_minutes
                    wait_minutes = 1
                    logger.info(f"TEST MODE: Using shortened wait time of {wait_minutes} minute(s) instead of {original_wait} minutes")
                logger.info(f"[Run {run_id}] Waiting for {wait_minutes} minutes for miners to fetch GFS and compute input hash...")
                await validator.update_task_status('weather', 'waiting', 'miner_fetch_wait')
                await asyncio.sleep(wait_minutes * 60)
                logger.info(f"[Run {run_id}] Wait finished. Proceeding with input hash verification.")
                await validator.update_task_status('weather', 'processing', 'verifying_hashes')
                await _update_run_status(self, run_id, "verifying_input_hashes")

                responses_to_check_query = """
                    SELECT id, miner_hotkey, job_id
                    FROM weather_miner_responses
                    WHERE run_id = :run_id AND status = 'fetch_initiated'
                """
                miners_to_poll = await self.db_manager.fetch_all(responses_to_check_query, {"run_id": run_id})
                logger.info(f"[Run {run_id}] Polling {len(miners_to_poll)} miners for input hash status.")

                miner_hash_results = {}
                polling_tasks = []

                async def _poll_single_miner(response_rec):
                    resp_id = response_rec['id']
                    miner_hk = response_rec['miner_hotkey']
                    miner_job_id = response_rec['job_id']
                    logger.debug(f"[Run {run_id}] Polling miner {miner_hk[:8]} (Job: {miner_job_id}) for input status using query_miners.")
                    
                    node = validator.metagraph.nodes.get(miner_hk)
                    if not node or not node.ip or not node.port:
                         logger.warning(f"[Run {run_id}] Miner {miner_hk[:8]} not found in metagraph or missing IP/Port. Cannot poll.")
                         return resp_id, {"status": "validator_poll_error", "message": "Miner not found in metagraph"}

                    try:
                        status_payload_data = WeatherGetInputStatusData(job_id=miner_job_id)
                        status_payload = {"nonce": str(uuid.uuid4()), "data": status_payload_data.model_dump()}
                        endpoint = "/weather-get-input-status"
                        
                        all_responses = await validator.query_miners(
                            payload=status_payload,
                            endpoint=endpoint,
                            hotkeys=[miner_hk]
                        )
                        
                        status_response = all_responses.get(miner_hk)
                        
                        if status_response:
                            parsed_response = status_response
                            if isinstance(status_response, dict) and 'text' in status_response:
                                try:
                                    parsed_response = json.loads(status_response['text'])
                                except (json.JSONDecodeError, TypeError) as json_err:
                                    logger.warning(f"[Run {run_id}] Failed to parse status response text for {miner_hk[:8]}: {json_err}")
                                    parsed_response = {"status": "parse_error", "message": str(json_err)}
                            
                            logger.debug(f"[Run {run_id}] Received status from {miner_hk[:8]}: {parsed_response}")
                            return resp_id, parsed_response
                        else:
                             logger.warning(f"[Run {run_id}] No response received from target miner {miner_hk[:8]} via query_miners.")
                             return resp_id, {"status": "validator_poll_failed", "message": "No response from miner via query_miners"}
                             
                    except Exception as poll_err:
                        logger.error(f"[Run {run_id}] Error polling miner {miner_hk[:8]}: {poll_err}", exc_info=True)
                        return resp_id, {"status": "validator_poll_error", "message": str(poll_err)}

                for resp_rec in miners_to_poll:
                    polling_tasks.append(_poll_single_miner(resp_rec))
                
                poll_results = await asyncio.gather(*polling_tasks)

                for resp_id, status_data in poll_results:
                    miner_hash_results[resp_id] = status_data
                logger.info(f"[Run {run_id}] Collected input status from {len(miner_hash_results)}/{len(miners_to_poll)} miners.")

                validator_input_hash = None
                try:
                    logger.info(f"[Run {run_id}] Validator computing its own reference input hash...")
                    gfs_cache_dir = Path(self.config.get('gfs_analysis_cache_dir', './gfs_analysis_cache'))
                    validator_input_hash = await compute_input_data_hash(
                        t0_run_time=gfs_t0_run_time,
                        t_minus_6_run_time=gfs_t_minus_6_run_time,
                        cache_dir=gfs_cache_dir
                    )
                    if validator_input_hash:
                        logger.info(f"[Run {run_id}] Validator computed reference hash: {validator_input_hash[:10]}...")
                    else:
                        logger.error(f"[Run {run_id}] Validator failed to compute its own reference input hash. Cannot verify miners.")
                        await _update_run_status(self, run_id, "error", "Validator failed hash computation")
                        miners_to_trigger = []

                except Exception as val_hash_err:
                    logger.error(f"[Run {run_id}] Error during validator hash computation: {val_hash_err}", exc_info=True)
                    await _update_run_status(self, run_id, "error", f"Validator hash error: {val_hash_err}")
                    miners_to_trigger = []

                miners_to_trigger = []
                if validator_input_hash:
                    update_tasks = []
                    for i, (resp_id, status_data) in enumerate(miner_hash_results.items()):
                        miner_status = status_data.get('status')
                        miner_hash = status_data.get('input_data_hash')
                        error_msg = status_data.get('message')
                        new_db_status = None
                        hash_match = None

                        if miner_hash and miner_status == 'input_hashed_awaiting_validation':
                            if miner_hash == validator_input_hash:
                                logger.info(f"[Run {run_id}] Hash MATCH for response ID {resp_id} (Miner status: {miner_status})!")
                                new_db_status = 'input_validation_complete'
                                hash_match = True
                                orig_rec = next((m for m in miners_to_poll if m['id'] == resp_id), None)
                                if orig_rec:
                                    miners_to_trigger.append((resp_id, orig_rec['miner_hotkey'], orig_rec['job_id']))
                                else:
                                     logger.error(f"[Run {run_id}] Could not find original record for resp_id {resp_id} to trigger inference.")
                            else:
                                logger.warning(f"[Run {run_id}] Hash MISMATCH for response ID {resp_id}. Miner: {miner_hash[:10]}... Validator: {validator_input_hash[:10]}... (Miner status: {miner_status})")
                                new_db_status = 'input_hash_mismatch'
                                hash_match = False
                        elif miner_status == 'fetch_error':
                            new_db_status = 'input_fetch_error'
                        elif miner_status in ['fetching_gfs', 'hashing_input', 'fetch_queued']:
                            logger.warning(f"[Run {run_id}] Miner for response ID {resp_id} timed out (status: {miner_status}).")
                            new_db_status = 'input_hash_timeout'
                        elif miner_status in ['validator_poll_failed', 'validator_poll_error']:
                             new_db_status = 'input_poll_error'
                        else:
                            new_db_status = 'input_fetch_error'

                        if new_db_status:
                            update_query = """
                                UPDATE weather_miner_responses
                                SET status = :status,
                                    input_hash_miner = :m_hash,
                                    input_hash_validator = :v_hash,
                                    input_hash_match = :match,
                                    error_message = :err,
                                    last_polled_time = :now
                                WHERE id = :resp_id
                            """
                            update_tasks.append(self.db_manager.execute(update_query, {
                                "resp_id": resp_id,
                                "status": new_db_status,
                                "m_hash": miner_hash,
                                "v_hash": validator_input_hash,
                                "match": hash_match,
                                "err": error_msg if error_msg is not None else "",
                                "now": datetime.now(timezone.utc)
                            }))
                    
                            # Yield control if processing many results
                            if len(miner_hash_results) > 20 and i % 20 == 19: # Yield every 20 items after the 20th
                                await asyncio.sleep(0)
                    
                    if update_tasks:
                         await asyncio.gather(*update_tasks)
                         logger.info(f"[Run {run_id}] Updated DB for {len(update_tasks)} miner responses after hash check.")
                 
                if miners_to_trigger:
                    logger.info(f"[Run {run_id}] Triggering inference for {len(miners_to_trigger)} miners with matching input hashes.")
                    await validator.update_task_status('weather', 'processing', 'triggering_inference')
                    trigger_tasks = []

                    async def _trigger_single_miner(resp_id, miner_hk, miner_job_id):
                        logger.debug(f"[Run {run_id}] Attempting to trigger inference for {miner_hk[:8]} (Job: {miner_job_id}) using query_miners.")
                        try:
                            trigger_payload_data = WeatherStartInferenceData(job_id=miner_job_id)
                            trigger_payload = {"nonce": str(uuid.uuid4()), "data": trigger_payload_data.model_dump()}
                            endpoint="/weather-start-inference"
                            
                            all_responses = await validator.query_miners(
                                payload=trigger_payload,
                                endpoint=endpoint,
                                hotkeys=[miner_hk]
                            )
                            
                            trigger_response = all_responses.get(miner_hk)
                            
                            parsed_response = trigger_response
                            if isinstance(trigger_response, dict) and 'text' in trigger_response:
                                try:
                                    parsed_response = json.loads(trigger_response['text'])
                                except (json.JSONDecodeError, TypeError) as json_err:
                                    logger.warning(f"[Run {run_id}] Failed to parse trigger response text for {miner_hk[:8]}: {json_err}")
                                    parsed_response = {"status": "parse_error", "message": str(json_err)}
                                    
                            if parsed_response and parsed_response.get('status') == 'inference_started':
                                logger.info(f"[Run {run_id}] Successfully triggered inference for {miner_hk[:8]} (Job: {miner_job_id}).")
                                return resp_id, True
                            else:
                                logger.warning(f"[Run {run_id}] Failed to trigger inference for {miner_hk[:8]} (Job: {miner_job_id}). Response: {parsed_response}")
                                return resp_id, False
                        except Exception as trigger_err:
                            logger.error(f"[Run {run_id}] Error triggering inference for {miner_hk[:8]} (Job: {miner_job_id}): {trigger_err}", exc_info=True)
                            return resp_id, False

                    for resp_id, miner_hk, miner_job_id in miners_to_trigger:
                        trigger_tasks.append(_trigger_single_miner(resp_id, miner_hk, miner_job_id))
                    
                    trigger_results = await asyncio.gather(*trigger_tasks)

                    final_update_tasks = []
                    triggered_count = 0
                    for i, (resp_id, success) in enumerate(trigger_results):
                        if success:
                            triggered_count += 1
                            final_update_tasks.append(self.db_manager.execute(
                                "UPDATE weather_miner_responses SET status = 'inference_triggered' WHERE id = :id",
                                {"id": resp_id}
                            ))
                        # Yield control if processing many results
                        if len(trigger_results) > 20 and i % 20 == 19: # Yield every 20 items after the 20th
                            await asyncio.sleep(0)
                    
                    if final_update_tasks:
                         await asyncio.gather(*final_update_tasks)
                    logger.info(f"[Run {run_id}] Completed inference trigger process. Successfully triggered {triggered_count}/{len(miners_to_trigger)} miners.")
                    if triggered_count > 0:
                        await _update_run_status(self, run_id, "awaiting_inference_results")
                    else:
                         await _update_run_status(self, run_id, "inference_trigger_failed") # No miners successfully triggered
                else:
                     logger.warning(f"[Run {run_id}] No miners eligible for inference trigger after hash verification.")
                     await _update_run_status(self, run_id, "no_matching_hashes")

                logger.info(f"[ValidatorExecute] Concluded processing for run {run_id}. Triggering validator_score.")
                await self.validator_score()

                if self.test_mode:
                    logger.info(f"TEST MODE: validator_execute iteration for run {run_id} complete.")
                    if hasattr(self, 'last_test_mode_run_id') and self.last_test_mode_run_id == run_id: 
                        logger.info(f"TEST MODE: Waiting for Day-1 scoring of run {run_id} to complete...")
                        try:
                            await asyncio.wait_for(self.test_mode_run_scored_event.wait(), timeout=600.0) # 10 min timeout
                            logger.info(f"TEST MODE: Day-1 scoring for run {run_id} event received.")
                        except asyncio.TimeoutError:
                            logger.error(f"TEST MODE: Timeout waiting for Day-1 scoring of run {run_id}.")
                        self.test_mode_run_scored_event.clear() 
                    else:
                        logger.info(f"TEST MODE: Not waiting for scoring event as run_id ({run_id}) doesn't match last_test_mode_run_id ({getattr(self, 'last_test_mode_run_id', None)}).")
                    logger.info("TEST MODE: Exiting validator loop after one successful attempt or error within the attempt.")
                    break

            except Exception as loop_err:
                 logger.error(f"Error in validator_execute main loop: {loop_err}", exc_info=True)
                 await validator.update_task_status('weather', 'error')
                 if 'run_id' in locals() and run_id is not None:
                      try: await _update_run_status(self, run_id, "error", error_message=f"Unhandled loop error: {loop_err}")
                      except: pass
                 
                 if self.test_mode:
                     logger.info("TEST MODE: Encountered an error. Exiting validator_execute loop.")
                     break 
                 
                 await asyncio.sleep(600) # Sleep only if not in test mode

    async def validator_score(self, result=None):
        """
        Initiates the verification process for completed miner responses.
        Actual scoring happens in background workers.
        """
        logger.info("Validator scoring check initiated...")
        
        verification_wait_minutes_actual = self.config.get('verification_wait_minutes', 30)
        if self.test_mode:
            logger.info("[validator_score] TEST MODE: Setting verification_wait_minutes to 0 for immediate processing.")
            verification_wait_minutes_actual = 0
            logger.info("[validator_score] TEST MODE: Adding a 30-second delay before processing runs for miner data preparation.")
            await asyncio.sleep(30)

        cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=verification_wait_minutes_actual)
        
        query = """
        SELECT id, gfs_init_time_utc 
        FROM weather_forecast_runs
        WHERE status = 'awaiting_inference_results' 
        AND run_initiation_time < :cutoff_time 
        ORDER BY run_initiation_time ASC
        LIMIT 10
        """
        forecast_runs = await self.db_manager.fetch_all(query, {"cutoff_time": cutoff_time})
        
        if not forecast_runs:
            logger.debug(f"No runs found awaiting inference results within cutoff (test_mode active: {self.test_mode}, cutoff: {cutoff_time}).")
            return
        
        for run_record in forecast_runs:
            run_id = run_record['id']
            logger.info(f"[Run {run_id}] Checking responses for verification...")
            current_run_status_rec = await self.db_manager.fetch_one("SELECT status FROM weather_forecast_runs WHERE id = :run_id", {"run_id": run_id})
            current_run_status = current_run_status_rec['status'] if current_run_status_rec else 'unknown'
            
            if current_run_status == 'awaiting_inference_results':
                await _update_run_status(self, run_id, "verifying_miner_forecasts") 
            else:
                 logger.info(f"[Run {run_id}] Status is already '{current_run_status}' (expected 'awaiting_inference_results'), skipping verification trigger step.")
                 continue
            
            responses_query = """
            SELECT mr.id, mr.miner_hotkey, mr.status, mr.job_id
            FROM weather_miner_responses mr
            WHERE mr.run_id = :run_id
            AND mr.status = 'inference_triggered'
            """
            miner_responses = await self.db_manager.fetch_all(responses_query, {"run_id": run_id})
            
            num_attempted_verification = len(miner_responses)
            if not miner_responses:
                logger.info(f"[Run {run_id}] No miner responses found with status 'inference_triggered'.")
            else:
                logger.info(f"[Run {run_id}] Found {num_attempted_verification} 'inference_triggered' responses to verify.")

            verification_tasks = []
            for response in miner_responses:
                 verification_tasks.append(verify_miner_response(self, run_record, response))
                 
            if verification_tasks:
                 await asyncio.gather(*verification_tasks)
                 logger.info(f"[Run {run_id}] Completed verification attempts for {len(verification_tasks)} responses.")
                 
            verified_responses_query = "SELECT COUNT(*) as count FROM weather_miner_responses WHERE run_id = :run_id AND verification_passed = TRUE"
            verified_count_result = await self.db_manager.fetch_one(verified_responses_query, {"run_id": run_id})
            verified_count = verified_count_result["count"] if verified_count_result else 0
            
            current_run_status_rec_after_verify = await self.db_manager.fetch_one("SELECT status FROM weather_forecast_runs WHERE id = :run_id", {"run_id": run_id})
            current_run_status_after_verify = current_run_status_rec_after_verify['status'] if current_run_status_rec_after_verify else 'unknown'
            
            if current_run_status_after_verify == 'verifying_miner_forecasts':
                if verified_count >= 1:
                    logger.info(f"[Run {run_id}] {verified_count} verified response(s). Triggering Day-1 QC scoring.")
                    await _trigger_initial_scoring(self, run_id)
                elif num_attempted_verification > 0: 
                     if verified_count == 0:
                         logger.warning(f"[Run {run_id}] No responses passed verification out of {num_attempted_verification} attempted. Status: all_forecasts_failed_verification.")
                         await _update_run_status(self, run_id, "all_forecasts_failed_verification")
                else:
                    logger.warning(f"[Run {run_id}] Run was '{current_run_status_after_verify}' but no 'inference_triggered' miner responses found to verify. Setting status to 'stalled_no_valid_forecasts'.")
                    await _update_run_status(self, run_id, "stalled_no_valid_forecasts")
            else:
                 logger.info(f"[Run {run_id}] Status changed from 'verifying_miner_forecasts' to '{current_run_status_after_verify}' during verification logic. No further status update needed here.")

    async def update_combined_weather_scores(self, run_id_trigger: Optional[int] = None):
        logger.info(f"[CombinedWeatherScore] Updating combined weather scores (triggered by run {run_id_trigger if run_id_trigger else 'periodic/manual call'}).")

        latest_day1_scores_array = np.full(256, 0.0)
        latest_era5_composite_scores_array = np.full(256, 0.0)
        
        active_uids = set()
        async with self.db_manager.session(operation_name="fetch_active_miner_uids_for_combined_score") as session:
            active_miners_query = "SELECT DISTINCT uid FROM node_table WHERE hotkey IS NOT NULL AND uid >= 0 AND uid < 256"
            result = await session.execute(text(active_miners_query))
            active_miner_uids_records = result.fetchall()
            active_uids = {rec.uid for rec in active_miner_uids_records}

        day1_qc_score_type = "day1_qc_score"
        query_day1_miner_scores = """
            SELECT score, calculation_time FROM weather_miner_scores 
            WHERE miner_uid = :miner_uid AND score_type = :score_type
            ORDER BY calculation_time DESC LIMIT 1
        """
        latest_day1_timestamp = None

        for uid in active_uids:
            async with self.db_manager.session(operation_name="fetch_latest_day1_score_for_uid") as session:
                result = await session.execute(text(query_day1_miner_scores), {"miner_uid": uid, "score_type": day1_qc_score_type})
                res_day1 = result.fetchone()
                if res_day1 and res_day1.score is not None and np.isfinite(res_day1.score):
                    latest_day1_scores_array[uid] = float(res_day1.score)
                    if latest_day1_timestamp is None or res_day1.calculation_time > latest_day1_timestamp:
                        latest_day1_timestamp = res_day1.calculation_time
        
        if latest_day1_timestamp:
            logger.info(f"[CombinedWeatherScore] Fetched latest Day-1 QC scores for {len(active_uids)} active UIDs, latest timestamp: {latest_day1_timestamp}.")
        else:
            logger.warning("[CombinedWeatherScore] No Day-1 QC scores found in weather_miner_scores for active UIDs.")
            latest_day1_timestamp = datetime.now(timezone.utc) 

        era5_composite_score_type = "era5_final_composite_score"
        query_era5_composite = """
            SELECT score FROM weather_miner_scores WHERE miner_uid = :miner_uid AND score_type = :score_type
            ORDER BY calculation_time DESC LIMIT 1
        """
        for uid in active_uids:
            async with self.db_manager.session(operation_name="fetch_latest_era5_composite_score_for_uid") as session:
                result = await session.execute(text(query_era5_composite), {"miner_uid": uid, "score_type": era5_composite_score_type})
                res_era5 = result.fetchone()
                if res_era5 and res_era5.score is not None and np.isfinite(res_era5.score):
                    latest_era5_composite_scores_array[uid] = float(res_era5.score)
        logger.info(f"[CombinedWeatherScore] Fetched ERA5 composite scores for {len(active_uids)} active UIDs.")

        W_day1 = self.config.get('weather_score_day1_weight', 0.2)
        W_era5 = self.config.get('weather_score_era5_weight', 0.8)
        proportional_weather_scores = (latest_day1_scores_array * W_day1) + (latest_era5_composite_scores_array * W_era5)
        logger.info(f"[CombinedWeatherScore] Calculated proportional scores.")

        miner_bonuses_applied = np.zeros(256)
        bonus_value_add = self.config.get('weather_bonus_value_add', 0.05)
        
        # Bonus Definitions
        bonus_metric_definitions = [
            {"id": "best_z500_acc_120h", "metric_score_type": "era5_acc_z500_120h", "higher_is_better": True, "min_threshold": 0.5},
            {"id": "lowest_t2m_rmse_120h", "metric_score_type": "era5_rmse_2t_120h", "higher_is_better": False, "min_threshold": 3.0},
            {"id": "best_msl_skill_gfs_168h", "metric_score_type": "era5_skill_gfs_msl_168h", "higher_is_better": True, "min_threshold": 0.1},
        ]

        for category in bonus_metric_definitions:
            metric_to_query = category["metric_score_type"]
            higher_is_better = category["higher_is_better"]
            min_thresh = category.get("min_threshold")

            query_bonus_metric = """ 
                SELECT miner_uid, score FROM weather_miner_scores
                WHERE score_type = :score_type AND miner_uid = :miner_uid AND score IS NOT NULL
                ORDER BY calculation_time DESC LIMIT 1
            """
            candidate_scores_for_category = []
            for uid_val in active_uids:
                metric_res = await self.db_manager.fetch_one(query_bonus_metric, {"score_type": metric_to_query, "miner_uid": uid_val})
                if metric_res and metric_res['score'] is not None and np.isfinite(metric_res['score']):
                    score = float(metric_res['score'])
                    eligible = True
                    if min_thresh is not None:
                        if higher_is_better and score < min_thresh:
                            eligible = False
                        elif not higher_is_better and score > min_thresh:
                            eligible = False
                    if eligible:
                        candidate_scores_for_category.append((score, uid_val))
            
            if not candidate_scores_for_category:
                logger.info(f"[CombinedWeatherScore] No eligible miners for bonus: {category['id']}")
                continue

            candidate_scores_for_category.sort(key=lambda x: x[0], reverse=higher_is_better)
            best_score = candidate_scores_for_category[0][0]
            winners = [uid for score, uid in candidate_scores_for_category if score == best_score]

            if 1 <= len(winners) <= 2:
                bonus_per_winner = bonus_value_add / len(winners)
                for winner_uid in winners:
                    miner_bonuses_applied[winner_uid] += bonus_per_winner
                    logger.info(f"[CombinedWeatherScore] Bonus for {category['id']} awarded to UID {winner_uid} (value: {bonus_per_winner:.3f}). Winning score: {best_score:.4f}")
            elif len(winners) > 2:
                logger.info(f"[CombinedWeatherScore] Bonus for {category['id']} skipped: too many winners ({len(winners)}) with score {best_score:.4f}.")
        
        final_weather_scores_for_table = np.clip(proportional_weather_scores + miner_bonuses_applied, 0.0, 1.1)
        logger.info(f"[CombinedWeatherScore] Applied bonuses. Max combined score: {np.max(final_weather_scores_for_table):.4f}")
        
        mock_evaluation_results = []
        for uid_idx in range(256):
            mock_evaluation_results.append({
                "miner_uid": uid_idx,
                "final_score_for_uid": final_weather_scores_for_table[uid_idx]
            })
        
        id_for_combined_row = "final_weather_scores"
        timestamp_for_combined_score_row = datetime.now(timezone.utc)
        
        await self.build_score_row(
            run_id = id_for_combined_row,
            gfs_init_time = timestamp_for_combined_score_row,
            evaluation_results = mock_evaluation_results,
            task_name_prefix = "weather"
        )
        logger.info(f"[CombinedWeatherScore] Update completed for 'weather' (task_id: {id_for_combined_row}).")

    ############################################################
    # Miner methods
    ############################################################

    async def miner_preprocess(
        self,
        data: Optional[Dict[str, Any]] = None,
    ) -> Optional[Batch]:
        """
        Loads and preprocesses the input GFS data payload received from the validator
        by calling the dedicated preprocessing function.
        
        Args:
            data: Dictionary containing the raw payload from the validator.

        Returns:
            An aurora.Batch object ready for model inference, or None if preprocessing fails.
        """
        logger.debug("Calling prepare_miner_batch_from_payload...")
        try:
            result_batch = await prepare_miner_batch_from_payload(data)
            logger.debug(f"prepare_miner_batch_from_payload returned: {type(result_batch)}")
            return result_batch
        except Exception as e:
             logger.error(f"Error calling prepare_miner_batch_from_payload: {e}", exc_info=True)
             return None

    async def miner_execute(self, data: Dict[str, Any], miner) -> Optional[Dict[str, Any]]:
        """
        Handles the initial request from the validator, preprocesses data,
        creates a job record, launches the background inference task,
        and returns an immediate 'Accepted' response.
        Checks for existing jobs for the same forecast time to avoid redundant runs.
        """
        logger.info("Miner execute called for WeatherTask")
        new_job_id = str(uuid.uuid4())

        if self.inference_runner is None:
             logger.error(f"[New Job Attempt {new_job_id}] Cannot execute: Inference Runner not available.")
             return {"status": "error", "message": "Miner inference component not ready"}
        if not data or 'data' not in data:
             logger.error(f"[New Job Attempt {new_job_id}] Invalid or missing payload data.")
             return {"status": "error", "message": "Invalid payload structure"}

        validator_hotkey = data.get("sender_hotkey", "unknown")
        payload_data = data['data']

        try:
            gfs_init_time = payload_data.get('forecast_start_time')
            if not isinstance(gfs_init_time, datetime):
                 try:
                     gfs_init_time_str = str(gfs_init_time)
                     if gfs_init_time_str.endswith('Z'):
                         gfs_init_time_str = gfs_init_time_str[:-1] + '+00:00'
                     gfs_init_time = datetime.fromisoformat(gfs_init_time_str)
                     if gfs_init_time.tzinfo is None:
                         gfs_init_time = gfs_init_time.replace(tzinfo=timezone.utc)
                     else:
                         gfs_init_time = gfs_init_time.astimezone(timezone.utc)

                 except (ValueError, TypeError) as parse_err:
                     logger.error(f"[New Job Attempt {new_job_id}] Invalid forecast_start_time format: {gfs_init_time}. Error: {parse_err}")
                     return {"status": "error", "message": f"Invalid forecast_start_time format: {parse_err}"}

            logger.info(f"Processing request for GFS init time: {gfs_init_time}")

            existing_job = await self.get_job_by_gfs_init_time(gfs_init_time)

            if existing_job:
                existing_job_id = existing_job['id']
                existing_status = existing_job['status']
                logger.info(f"[Job {existing_job_id}] Found existing {existing_status} job for GFS init time {gfs_init_time}. Reusing this job ID.")
                return {"status": "accepted", "job_id": existing_job_id, "message": f"Accepted. Reusing existing {existing_status} job."}

            job_id = new_job_id
            logger.info(f"[Job {job_id}] No suitable existing job found. Creating new job for GFS init time {gfs_init_time}.")

            logger.info(f"[Job {job_id}] Starting preprocessing...")
            preprocessing_start_time = time.time()
            initial_batch = await self.miner_preprocess(data=payload_data)
            if initial_batch is None:
                logger.error(f"[Job {job_id}] Preprocessing failed.")
                return {"status": "error", "message": "Failed to preprocess input data"}
            logger.info(f"[Job {job_id}] Preprocessing completed in {time.time() - preprocessing_start_time:.2f} seconds.")

            logger.info(f"[Job {job_id}] Creating initial job record in database.")
            insert_query = """
                INSERT INTO weather_miner_jobs (id, validator_request_time, validator_hotkey, gfs_init_time_utc, gfs_input_metadata, status, processing_start_time)
                VALUES (:id, :req_time, :val_hk, :gfs_init, :gfs_meta, :status, :proc_start)
            """
            if gfs_init_time.tzinfo is None:
                 gfs_init_time = gfs_init_time.replace(tzinfo=timezone.utc)

            await self.db_manager.execute(insert_query, {
                "id": job_id,
                "req_time": datetime.now(timezone.utc),
                "val_hk": validator_hotkey,
                "gfs_init": gfs_init_time,
                "gfs_meta": json.dumps(payload_data, default=str),
                "status": "received",
                "proc_start": datetime.now(timezone.utc)
            })
            logger.info(f"[Job {job_id}] Initial job record created.")

            logger.info(f"[Job {job_id}] Launching background inference task...")

            return {"status": "accepted", "job_id": job_id, "message": "Weather forecast job accepted for processing."}

        except Exception as e:
            job_id_for_error = new_job_id
            logger.error(f"[Job {job_id_for_error}] Error during initial miner_execute: {e}", exc_info=True)
            return {"status": "error", "job_id": None, "message": f"Failed to initiate job: {e}"}

    async def handle_kerchunk_request(self, job_id: str) -> Dict[str, Any]:
        """
        Handle a request for forecast data for a specific job.
        Now returns information about the Zarr store directly instead of Kerchunk JSON.
        
        Args:
            job_id: The unique identifier for the job
            
        Returns:
            Dict containing status, message, and if completed:
            - zarr_store_url: URL to access the Zarr store
            - verification_hash: Hash to verify forecast integrity
            - access_token: JWT token for accessing forecast files
        """            
        logger.info(f"Handling forecast data request for job_id: {job_id}")
        
        try:
            query = """
            SELECT id as job_id, status, target_netcdf_path, verification_hash, error_message
            FROM weather_miner_jobs
            WHERE id = :job_id 
            """
            job = await self.db_manager.fetch_one(query, {"job_id": job_id})
            
            if not job:
                logger.warning(f"Job not found for job_id: {job_id}")
                return {"status": "not_found", "message": f"Job with ID {job_id} not found"}
                
            if job["status"] == "completed":
                zarr_path_str = job["target_netcdf_path"]
                verification_hash = job["verification_hash"]
                
                if not zarr_path_str or not verification_hash:
                    logger.error(f"Job {job_id} completed but missing required path/hash info. Zarr path: {zarr_path_str}, Hash: {verification_hash}")
                    return {"status": "error", "message": "Job completed but data paths or hash missing"}
                
                zarr_path = Path(zarr_path_str)
                zarr_dir_name = zarr_path.name
                
                if not zarr_dir_name.endswith(".zarr"):
                    logger.error(f"Expected Zarr directory but found: {zarr_dir_name}")
                    return {"status": "error", "message": "Invalid Zarr store format"}

                logger.info(f"[Job {job_id}] Using Zarr directory for JWT claim: {zarr_dir_name}")

                miner_jwt_secret_key = self.config.get('miner_jwt_secret_key', os.getenv("MINER_JWT_SECRET_KEY"))
                if not miner_jwt_secret_key:
                    logger.warning("MINER_JWT_SECRET_KEY not set in config or environment. Using default insecure key.")
                    miner_jwt_secret_key = "insecure_default_key_for_development_only"
                jwt_algorithm = self.config.get('jwt_algorithm', "HS256")
                token_expire_minutes = int(self.config.get('access_token_expire_minutes', 60))
                
                token_data = {
                    "job_id": job_id,
                    "file_path": zarr_dir_name,
                    "exp": datetime.now(timezone.utc) + timedelta(minutes=token_expire_minutes)
                }
                
                access_token = jwt.encode(
                    token_data,
                    miner_jwt_secret_key, 
                    algorithm=jwt_algorithm
                )
                
                zarr_url_for_response = f"/forecasts/{zarr_dir_name}"
                
                return {
                    "status": "completed",
                    "message": "Forecast completed and ready for access",
                    "zarr_store_url": zarr_url_for_response,
                    "verification_hash": verification_hash,
                    "access_token": access_token
                }
                
            elif job["status"] == "error":
                logger.warning(f"[Job {job_id}] Forecast data requested but job failed. Error: {job['error_message'] or 'Unknown error'}")
                return {"status": "error", "message": f"Job failed: {job['error_message'] or 'Unknown error'}"}
            else:
                logger.info(f"[Job {job_id}] Forecast data requested but job still processing (status: {job['status']}). Validator will need to retry later.")
                return {"status": "processing", "message": f"Job is currently in status: {job['status']}"}
                
        except Exception as e:
            logger.error(f"Error handling forecast data request for job_id {job_id}: {e}", exc_info=True)
            return {"status": "error", "message": f"Failed to process request: {str(e)}"}

    async def handle_initiate_fetch(self, request_data: 'WeatherInitiateFetchData') -> Dict[str, Any]:
        """
        Handles the /weather-initiate-fetch request.
        Creates a job record and launches the background task for fetching GFS and hashing.
        If a failed job for the same timestep exists, it will be reset and retried.
        """
        if self.node_type != 'miner':
            logger.error("handle_initiate_fetch called on non-miner node.")
            return {"status": "error", "job_id": None, "message": "Invalid node type"}

        try:
            t0_run_time = request_data.forecast_start_time
            t_minus_6_run_time = request_data.previous_step_time

            if t0_run_time.tzinfo is None: t0_run_time = t0_run_time.replace(tzinfo=timezone.utc)
            if t_minus_6_run_time.tzinfo is None: t_minus_6_run_time = t_minus_6_run_time.replace(tzinfo=timezone.utc)

            logger.info(f"[Miner] Received initiate_fetch request for T0={t0_run_time}")

            # Find any existing job for this exact time, regardless of status
            existing_job_query = """
                SELECT id, status, input_data_hash
                FROM weather_miner_jobs 
                WHERE gfs_init_time_utc = :gfs_init
                AND gfs_t_minus_6_time_utc = :gfs_t_minus_6
                ORDER BY validator_request_time DESC
                LIMIT 1
            """
            
            existing_job = await self.db_manager.fetch_one(existing_job_query, {
                "gfs_init": t0_run_time,
                "gfs_t_minus_6": t_minus_6_run_time
            })

            if existing_job:
                job_id = existing_job['id']
                status = existing_job['status']
                
                # If the job is in a recoverable (failed) state, reset and retry it.
                if status in ['error', 'fetch_error', 'failed', 'input_hash_mismatch', 'input_hash_timeout', 'input_poll_error']:
                    logger.warning(f"[Miner Job {job_id}] Found existing FAILED job (status: {status}). Resetting and retrying.")
                    
                    # Reset status to 'fetch_queued' and clear any previous error message.
                    await update_job_status(self, job_id, 'fetch_queued', error_message="")
                    
                    # Relaunch the background task to fetch and hash the data.
                    asyncio.create_task(fetch_and_hash_gfs_task(
                        task_instance=self,
                        job_id=job_id,
                        t0_run_time=t0_run_time,
                        t_minus_6_run_time=t_minus_6_run_time
                    ))
                    
                    return {"status": "fetch_accepted", "job_id": job_id, "message": f"Retrying existing failed job (previous status: {status})."}

                # If the job is in a valid, non-failed state, reuse it.
                else:
                    logger.info(f"[Miner Job {job_id}] Found existing active job (status: {status}). Reusing.")
                    response = {
                        "status": "fetch_accepted", 
                        "job_id": job_id, 
                        "message": f"Reusing existing job (status: {status})"
                    }
                    if existing_job.get('input_data_hash'):
                        response["input_data_hash"] = existing_job['input_data_hash']
                    return response

            # If no job exists, create a new one.
            job_id = str(uuid.uuid4())
            logger.info(f"[Miner Job {job_id}] No existing job found. Creating new job for T0={t0_run_time}.")

            insert_query = """
                INSERT INTO weather_miner_jobs
                (id, validator_request_time, gfs_init_time_utc, gfs_t_minus_6_time_utc, status)
                VALUES (:id, :req_time, :gfs_init, :gfs_t_minus_6, :status)
            """
            await self.db_manager.execute(insert_query, {
                "id": job_id,
                "req_time": datetime.now(timezone.utc),
                "gfs_init": t0_run_time,
                "gfs_t_minus_6": t_minus_6_run_time,
                "status": "fetch_queued"
            })
            logger.info(f"[Miner Job {job_id}] DB record created. Launching background fetch/hash task.")

            asyncio.create_task(fetch_and_hash_gfs_task(
                task_instance=self,
                job_id=job_id,
                t0_run_time=t0_run_time,
                t_minus_6_run_time=t_minus_6_run_time
            ))

            return {"status": "fetch_accepted", "job_id": job_id, "message": "Fetch and hash process initiated."}

        except Exception as e:
            logger.error(f"[Miner] Error during handle_initiate_fetch: {e}", exc_info=True)
            return {"status": "error", "job_id": None, "message": f"Failed to initiate fetch: {e}"}

    async def handle_get_input_status(self, job_id: str) -> Dict[str, Any]:
        """
        Handles the /weather-get-input-status request.
        Returns the current status and input hash (if computed) for the job.
        If the job is ready for inference but hasn't been triggered, it reports
        the status as 'input_hashed_awaiting_validation' to conform to the validator.
        """
        if self.node_type != 'miner':
            logger.error("handle_get_input_status called on non-miner node.")
            return {"status": "error", "job_id": job_id, "message": "Invalid node type"}

        logger.debug(f"[Miner Job {job_id}] Received get_input_status request.")
        try:
            query = "SELECT status, input_data_hash, error_message FROM weather_miner_jobs WHERE id = :job_id"
            result = await self.db_manager.fetch_one(query, {"job_id": job_id})

            if not result:
                logger.warning(f"[Miner Job {job_id}] Status requested for non-existent job.")
                return {"status": "not_found", "job_id": job_id, "message": "Job ID not found."}
            
            status_to_report = result['status']
            # If hashing is done and we are waiting for the validator to trigger inference,
            # report the specific status the validator is looking for.
            if status_to_report == 'in_progress' and result.get('input_data_hash'):
                status_to_report = 'input_hashed_awaiting_validation'

            response = {
                "job_id": job_id,
                "status": status_to_report,
                "input_data_hash": result.get('input_data_hash'),
                "message": result.get('error_message')
            }
            logger.debug(f"[Miner Job {job_id}] Reporting status: {response['status']}, Hash available: {response['input_data_hash'] is not None}")
            return response

        except Exception as e:
            logger.error(f"[Miner Job {job_id}] Error during handle_get_input_status: {e}", exc_info=True)
            return {"status": "error", "job_id": job_id, "message": f"Failed to get status: {e}"}

    async def handle_start_inference(self, job_id: str) -> Dict[str, Any]:
        """
        Handles a request from a validator to start the inference process for a given job_id.
        """
        if not job_id:
            return {"status": "error", "message": "job_id is required."}

        logger.info(f"Miner received request to start inference for job_id: {job_id}")

        try:
            async with self.gpu_semaphore:
                logger.info(f"GPU semaphore acquired for job_id: {job_id}.")

                # Check job status and existing runpod_job_id to prevent duplicates
                job_query = "SELECT status, runpod_job_id FROM weather_miner_jobs WHERE id = :job_id"
                job_details = await self.db_manager.fetch_one(job_query, {"job_id": job_id})

                if not job_details:
                    return {"status": "error", "message": f"Job ID {job_id} not found."}

                if job_details['runpod_job_id'] and job_details['status'] == 'in_progress':
                    logger.warning(f"[{job_id}] Received start_inference call for a job already in progress on RunPod. Acknowledging.")
                    return {"status": "success", "message": "Inference already in progress."}

                inference_type = self.config.get('weather_inference_type', 'local_model')
                logger.info(f"Executing inference for job {job_id} using type: {inference_type}")

                if inference_type == "http_service":
                    # This function now returns True if the async poller was launched successfully
                    success = await self._run_inference_via_http_service(job_id)
                    if success:
                        return {"status": "success", "message": "Inference process started."}
                    else:
                        return {"status": "error", "message": "Failed to start inference process."}

                elif inference_type == "local_model":
                    if not self.inference_runner or not self.inference_runner.model:
                        msg = "Local model selected but not loaded."
                        logger.error(f"[{job_id}] {msg}")
                        await update_job_status(self, job_id, 'failed', msg)
                        return {"status": "error", "message": msg}
                    
                    await run_inference_background(task_instance=self, job_id=job_id)
                    job_status_rec = await self.db_manager.fetch_one("SELECT status FROM weather_miner_jobs WHERE id = :job_id", {"job_id": job_id})
                    final_status = job_status_rec['status'] if job_status_rec else 'unknown'

                    if final_status == 'completed':
                        return {"status": "success", "message": "Inference process completed."}
                    else:
                        return {"status": "error", "message": "Inference process failed."}
                
                else:
                    msg = f"Unsupported inference type: {inference_type}"
                    logger.error(f"[{job_id}] {msg}")
                    await update_job_status(self, job_id, 'failed', msg)
                    return {"status": "error", "message": msg}

        except Exception as e:
            logger.error(f"Unexpected error in handle_start_inference for job {job_id}: {e}", exc_info=True)
            await update_job_status(self, job_id, 'failed', f"Unhandled exception: {e}")
            return {"status": "error", "message": f"An unexpected error occurred: {e}"}

    ############################################################
    # Helper Methods
    ############################################################

    async def cleanup_resources(self):
        """
        Cleans up old files from the local filesystem.
        """
        logger.info("Cleaning up weather task resources...")
        
        # Stop all background workers with proper awaiting
        logger.info("Stopping all background workers...")
        try:
            await self.stop_background_workers()
        except Exception as e:
            logger.error(f"Error during background worker cleanup: {e}")
        
        # Clean up ERA5 climatology dataset
        try:
            if hasattr(self, 'era5_climatology_ds') and self.era5_climatology_ds is not None:
                logger.info("Closing ERA5 climatology dataset...")
                self.era5_climatology_ds.close()
                self.era5_climatology_ds = None
                logger.info("Closed ERA5 climatology dataset")
        except Exception as e:
            logger.warning(f"Error closing ERA5 climatology dataset: {e}")

        # Clean up any HTTP clients
        try:
            if hasattr(self, 'validator') and self.validator and hasattr(self.validator, 'miner_client'):
                logger.info("Closing validator HTTP clients...")
                if not self.validator.miner_client.is_closed:
                    await self.validator.miner_client.aclose()
                logger.info("Closed validator HTTP clients")
        except Exception as e:
            logger.warning(f"Error closing HTTP clients: {e}")

        # Clean up fsspec/gcsfs caches and sessions
        try:
            logger.info("Clearing fsspec filesystem cache...")
            # Suppress fsspec warnings during shutdown
            import logging
            logging.getLogger('fsspec').setLevel(logging.ERROR)
            logging.getLogger('gcsfs').setLevel(logging.ERROR)
            logging.getLogger('aiohttp').setLevel(logging.ERROR)
            
            # Clear fsspec registry and cache to prevent session cleanup issues
            fsspec.config.conf.clear()
            if hasattr(fsspec.filesystem, '_cache'):
                fsspec.filesystem._cache.clear()
            logger.info("Cleared fsspec caches")
        except Exception as e:
            logger.warning(f"Error clearing fsspec caches: {e}")

        # Force garbage collection to help with cleanup
        try:
            import gc
            collected = gc.collect()
            logger.info(f"Garbage collection freed {collected} objects")
        except Exception as e:
            logger.warning(f"Error during garbage collection: {e}")

        logger.info("Weather task cleanup completed")

    async def start_initial_scoring_workers(self, num_workers=1):
        if self.node_type != "validator":
            return
        logger.info(f"[WeatherTask] Attempting to start {num_workers} initial scoring worker(s). Currently {len(self.initial_scoring_workers)} active.")
        if self.initial_scoring_worker_running and len(self.initial_scoring_workers) >= num_workers:
            logger.info(f"[WeatherTask] Initial scoring worker(s) already running ({len(self.initial_scoring_workers)}/{num_workers}). Not starting more.")
            return
        
        self.initial_scoring_worker_running = True
        needed_workers = num_workers - len(self.initial_scoring_workers)
        for i in range(needed_workers):
            worker_task = asyncio.create_task(initial_scoring_worker(self))
            self.initial_scoring_workers.append(worker_task)
            logger.info(f"[WeatherTask] Started initial_scoring_worker task {i+1}/{needed_workers} (Total active: {len(self.initial_scoring_workers)}). Task ID: {id(worker_task)}")

    async def stop_initial_scoring_workers(self):
        if self.node_type != "validator":
            return
        logger.info("Stopping initial scoring workers...")
        for worker in self.initial_scoring_workers:
            worker.cancel()
            
        self.initial_scoring_workers = []
        logger.info("Stopped all initial scoring workers")
        
    async def start_final_scoring_workers(self, num_workers=1):
        """Start background workers for final ERA5-based scoring."""
        if self.final_scoring_worker_running:
            logger.info("Final scoring workers already running")
            return
            
        self.final_scoring_worker_running = True
        for _ in range(num_workers):
            worker = asyncio.create_task(finalize_scores_worker(self))
            self.final_scoring_workers.append(worker)
        logger.info(f"Started {num_workers} final scoring workers")
        
    async def stop_final_scoring_workers(self):
        """Stop all background final scoring workers."""
        if not self.final_scoring_worker_running:
            return
            
        self.final_scoring_worker_running = False
        logger.info("Stopping final scoring workers...")
        for worker in self.final_scoring_workers:
            worker.cancel()
            
        self.final_scoring_workers = []
        logger.info("Stopped all final scoring workers")
        
    async def start_cleanup_workers(self, num_workers=1):
        """Start background workers for cleaning up old data."""
        if self.cleanup_worker_running:
            logger.info("Cleanup workers already running")
            return
            
        self.cleanup_worker_running = True
        for _ in range(num_workers):
            worker = asyncio.create_task(cleanup_worker(self))
            self.cleanup_workers.append(worker)
            
        logger.info(f"Started {num_workers} cleanup workers")
        
    async def stop_cleanup_workers(self):
        """Stop all background cleanup workers."""
        if not self.cleanup_worker_running:
            return
            
        self.cleanup_worker_running = False
        logger.info("Stopping cleanup workers...")
        for worker in self.cleanup_workers:
            worker.cancel()
            
        self.cleanup_workers = []
        logger.info("Stopped all cleanup workers")
        
    async def start_background_workers(self, num_ensemble_workers=1, num_initial_scoring_workers=1, num_final_scoring_workers=1, num_cleanup_workers=1):
         """Starts all background worker types."""
         await self.start_initial_scoring_workers(num_initial_scoring_workers)
         await self.start_final_scoring_workers(num_final_scoring_workers)
         await self.start_cleanup_workers(num_cleanup_workers)
         
    async def stop_background_workers(self):
        """Stops all background worker types with proper task cleanup."""
        logger.info("Stopping all background workers...")

        async def _stop_worker_list(worker_list, worker_name):
            """Helper to properly stop a list of worker tasks."""
            if not worker_list:
                return
            
            logger.info(f"Stopping {len(worker_list)} {worker_name} worker(s)...")
            
            # Cancel all tasks
            for worker in worker_list:
                if not worker.done():
                    worker.cancel()
            
            # Wait for cancellation to complete with timeout
            if worker_list:
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*worker_list, return_exceptions=True),
                        timeout=5.0
                    )
                    logger.info(f"Successfully stopped {worker_name} workers")
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout stopping {worker_name} workers, forcing cleanup")
                except Exception as e:
                    logger.warning(f"Error stopping {worker_name} workers: {e}")

        # Stop each worker type
        try: 
            await _stop_worker_list(self.initial_scoring_workers, "initial_scoring")
            self.initial_scoring_workers = []
            self.initial_scoring_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping initial scoring workers: {e}")
            
        try: 
            await _stop_worker_list(self.final_scoring_workers, "final_scoring")
            self.final_scoring_workers = []
            self.final_scoring_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping final scoring workers: {e}")
            
        try: 
            await _stop_worker_list(self.cleanup_workers, "cleanup")
            self.cleanup_workers = []
            self.cleanup_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping cleanup workers: {e}")

        logger.info("All background workers stopped")
        
    async def miner_fetch_hash_worker(self):
        """Worker that periodically checks for jobs awaiting input hash verification."""
        CHECK_INTERVAL_SECONDS = 10 if self.test_mode else 60
        
        while getattr(self, 'miner_fetch_hash_worker_running', False):
            try:
                logger.info("Miner fetch hash worker checking for runs awaiting hash verification...")
                
                query = """
                SELECT id, gfs_init_time_utc
                FROM weather_forecast_runs 
                WHERE status = 'awaiting_input_hashes'
                ORDER BY run_initiation_time ASC
                LIMIT 5
                """

                runs = await self.db_manager.fetch_all(query)
                if not runs:
                    logger.debug(f"No runs awaiting hash verification. Sleeping for {CHECK_INTERVAL_SECONDS}s...")
                    await asyncio.sleep(CHECK_INTERVAL_SECONDS)
                    continue
                    
                for run in runs:
                    run_id = run['id']
                    gfs_init_time = run['gfs_init_time_utc']
                    current_time = datetime.now(timezone.utc)
                    elapsed_minutes = (current_time - gfs_init_time).total_seconds() / 60
                    
                    min_wait_minutes = getattr(self.config, 'verification_wait_minutes', 30)
                    
                    if elapsed_minutes < min_wait_minutes and not self.test_mode:
                        logger.info(f"[HashWorker] [Run {run_id}] Only {elapsed_minutes:.1f} minutes elapsed, "
                                    f"waiting for minimum {min_wait_minutes} minutes")
                        continue
                    elif self.test_mode and elapsed_minutes < 0.1:
                        logger.info(f"[HashWorker] [Run {run_id}] TEST MODE: Minimal wait of 0.1 minutes")
                        await asyncio.sleep(6)
                    
                    responses_query = """
                    SELECT id, miner_hotkey, job_id
                    FROM weather_miner_responses
                    WHERE run_id = :run_id
                    AND status = 'fetch_initiated'
                    """
                    miners_to_poll = await self.db_manager.fetch_all(responses_query, {"run_id": run_id})
                    logger.info(f"[HashWorker] [Run {run_id}] Polling {len(miners_to_poll)} miners for input hash status.")

                    for resp_rec in miners_to_poll:
                        resp_id = resp_rec['id']
                        miner_hk = resp_rec['miner_hotkey']
                        miner_job_id = resp_rec['job_id']
                        logger.debug(f"[HashWorker] [Run {run_id}] Querying miner {miner_hk[:8]} (Job: {miner_job_id}) for input status.")
                        try:
                            status_payload_data = WeatherGetInputStatusData(job_id=miner_job_id)
                            status_payload = {"nonce": str(uuid.uuid4()), "data": status_payload_data.model_dump()}
                            
                            responses_dict = await self.query_miners(
                                payload=status_payload,
                                endpoint="/weather-get-input-status",
                                hotkeys=[miner_hk] 
                            )
                            
                            status_response = responses_dict.get(miner_hk)

                            if status_response:
                                parsed_response = status_response
                                if isinstance(status_response, dict) and 'text' in status_response:
                                    try:
                                        parsed_response = json.loads(status_response['text'])
                                    except (json.JSONDecodeError, TypeError) as json_err:
                                        logger.warning(f"[Run {run_id}] Failed to parse status response text for {miner_hk[:8]}: {json_err}")
                                        parsed_response = {"status": "parse_error", "message": str(json_err)}
                                
                                logger.debug(f"[Run {run_id}] Received status from {miner_hk[:8]}: {parsed_response}")
                                return resp_id, parsed_response
                            else:
                                logger.warning(f"[Run {run_id}] No valid response received from {miner_hk[:8]} using query_miners.")
                                return resp_id, {"status": "validator_poll_failed", "message": "No response from miner via query_miners"}
                        except Exception as poll_err:
                            logger.error(f"[Run {run_id}] Error polling miner {miner_hk[:8]}: {poll_err}", exc_info=True)
                            return resp_id, {"status": "validator_poll_error", "message": str(poll_err)}

            except Exception as e:
                logger.error(f"Error in miner_fetch_hash_worker: {e}", exc_info=True)
                await asyncio.sleep(60)
            
    async def stop_r2_cleanup_workers(self):
        await self._stop_worker_list(self.r2_cleanup_workers, "R2 Cleanup")
        self.r2_cleanup_worker_running = False

    async def start_r2_cleanup_workers(self, num_workers=1):
        if self.r2_cleanup_worker_running:
            logger.info("R2 cleanup workers are already running.")
            return
        self.r2_cleanup_worker_running = True
        for i in range(num_workers):
            task = asyncio.create_task(r2_cleanup_worker(self))
            self.r2_cleanup_workers.append(task)
        logger.info(f"Started {num_workers} R2 cleanup workers.")

    async def start_background_workers(self, num_ensemble_workers=1, num_initial_scoring_workers=1, num_final_scoring_workers=1, num_cleanup_workers=1):
        if self.node_type == "validator":
            await self.start_initial_scoring_workers(num_workers=num_initial_scoring_workers)
            await self.start_final_scoring_workers(num_workers=num_final_scoring_workers)
            await self.start_cleanup_workers(num_workers=num_cleanup_workers)
        elif self.node_type == "miner":
            if self.config.get('r2_cleanup_enabled', False):
                await self.start_r2_cleanup_workers(num_workers=1)
            else:
                logger.info("R2 cleanup worker is disabled by configuration.")


    async def stop_background_workers(self):
        if self.node_type == "validator":
            await self.stop_initial_scoring_workers()
            await self.stop_final_scoring_workers()
            await self.stop_cleanup_workers()
        elif self.node_type == "miner":
            if self.r2_cleanup_worker_running:
                await self.stop_r2_cleanup_workers()

        async def _stop_worker_list(worker_list, worker_name):
            if not worker_list:
                return
            
            logger.info(f"Stopping {len(worker_list)} {worker_name} worker(s)...")
            
            # Cancel all tasks
            for worker in worker_list:
                if not worker.done():
                    worker.cancel()
            
            # Wait for cancellation to complete with timeout
            if worker_list:
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*worker_list, return_exceptions=True),
                        timeout=5.0
                    )
                    logger.info(f"Successfully stopped {worker_name} workers")
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout stopping {worker_name} workers, forcing cleanup")
                except Exception as e:
                    logger.warning(f"Error stopping {worker_name} workers: {e}")

        # Stop each worker type
        try: 
            await _stop_worker_list(self.initial_scoring_workers, "initial_scoring")
            self.initial_scoring_workers = []
            self.initial_scoring_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping initial scoring workers: {e}")
            
        try: 
            await _stop_worker_list(self.final_scoring_workers, "final_scoring")
            self.final_scoring_workers = []
            self.final_scoring_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping final scoring workers: {e}")
            
        try: 
            await _stop_worker_list(self.cleanup_workers, "cleanup")
            self.cleanup_workers = []
            self.cleanup_worker_running = False
        except Exception as e: 
            logger.error(f"Error stopping cleanup workers: {e}")

        logger.info("All background workers stopped")
        
    async def _load_batch_from_db(self, job_id: str) -> Optional[Batch]:
        """Loads the initial batch object from the pickle file path stored in the database."""
        logger.info(f"[{job_id}] Loading initial batch from database path.")
        try:
            query = "SELECT input_batch_pickle_path FROM weather_miner_jobs WHERE id = :job_id"
            result = await self.db_manager.fetch_one(query, {"job_id": job_id})

            if not result or not result['input_batch_pickle_path']:
                logger.error(f"[{job_id}] No input_batch_pickle_path found in DB for this job.")
                return None
            
            pickle_path = Path(result['input_batch_pickle_path'])
            if not pickle_path.exists():
                logger.error(f"[{job_id}] Pickle file not found at path from DB: {pickle_path}")
                return None

            with open(pickle_path, "rb") as f:
                initial_batch = pickle.load(f)
            
            logger.info(f"[{job_id}] Successfully loaded batch from {pickle_path}.")
            return initial_batch

        except Exception as e:
            logger.error(f"[{job_id}] Error loading batch from DB path: {e}", exc_info=True)
            return None

    async def recover_incomplete_http_jobs(self):
        """
        Finds jobs that were in-progress on RunPod and restarts the polling worker for them.
        This should be called on miner startup.
        """
        if self.node_type != "miner" or self.config.get('weather_inference_type') != 'http_service':
            return
        
        logger.info("Checking for incomplete HTTP jobs to recover...")
        try:
            query = "SELECT id, runpod_job_id FROM weather_miner_jobs WHERE status = 'in_progress' AND runpod_job_id IS NOT NULL"
            incomplete_jobs = await self.db_manager.fetch_all(query)
            
            if not incomplete_jobs:
                logger.info("No incomplete HTTP jobs found.")
                return

            logger.info(f"Found {len(incomplete_jobs)} incomplete HTTP job(s). Restarting pollers...")
            for job in incomplete_jobs:
                job_id = job['id']
                runpod_job_id = job['runpod_job_id']
                logger.info(f"  - Restarting poller for Job ID: {job_id}, RunPod ID: {runpod_job_id}")
                asyncio.create_task(poll_runpod_job_worker(self, job_id, runpod_job_id))
        except Exception as e:
            logger.error(f"Error during incomplete HTTP job recovery: {e}", exc_info=True)
        